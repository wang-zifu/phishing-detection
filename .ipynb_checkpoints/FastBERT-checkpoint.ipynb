{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fast-bert\n",
      "  Downloading fast_bert-1.6.4-py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 381 kB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: tensorboardX in /usr/local/lib/python3.6/dist-packages (from fast-bert) (2.0)\n",
      "Requirement already satisfied, skipping upgrade: spacy in /usr/local/lib/python3.6/dist-packages (from fast-bert) (2.2.3)\n",
      "Requirement already satisfied, skipping upgrade: sklearn in /usr/local/lib/python3.6/dist-packages (from fast-bert) (0.0)\n",
      "Requirement already satisfied, skipping upgrade: fastprogress in /usr/local/lib/python3.6/dist-packages (from fast-bert) (0.2.2)\n",
      "Requirement already satisfied, skipping upgrade: transformers>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from fast-bert) (2.5.1)\n",
      "Requirement already satisfied, skipping upgrade: pytorch-lamb in /usr/local/lib/python3.6/dist-packages (from fast-bert) (1.0.0)\n",
      "Requirement already satisfied, skipping upgrade: python-box in /usr/local/lib/python3.6/dist-packages (from fast-bert) (4.2.1)\n",
      "Requirement already satisfied, skipping upgrade: tokenizers in /usr/local/lib/python3.6/dist-packages (from fast-bert) (0.5.2)\n",
      "Requirement already satisfied, skipping upgrade: pandas in /usr/local/lib/python3.6/dist-packages (from fast-bert) (0.25.3)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX->fast-bert) (3.11.2)\n",
      "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX->fast-bert) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX->fast-bert) (1.18.1)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy->fast-bert) (45.1.0)\n",
      "Requirement already satisfied, skipping upgrade: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->fast-bert) (0.4.1)\n",
      "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy->fast-bert) (1.0.2)\n",
      "Requirement already satisfied, skipping upgrade: srsly<1.1.0,>=0.1.0 in /usr/local/lib/python3.6/dist-packages (from spacy->fast-bert) (1.0.2)\n",
      "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy->fast-bert) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->fast-bert) (0.6.0)\n",
      "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->fast-bert) (2.0.3)\n",
      "Requirement already satisfied, skipping upgrade: thinc<7.4.0,>=7.3.0 in /usr/local/lib/python3.6/dist-packages (from spacy->fast-bert) (7.3.1)\n",
      "Requirement already satisfied, skipping upgrade: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy->fast-bert) (1.0.0)\n",
      "Requirement already satisfied, skipping upgrade: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy->fast-bert) (1.1.3)\n",
      "Requirement already satisfied, skipping upgrade: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->fast-bert) (3.0.2)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn->fast-bert) (0.22.1)\n",
      "Requirement already satisfied, skipping upgrade: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.3.0->fast-bert) (1.12.15)\n",
      "Requirement already satisfied, skipping upgrade: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.3.0->fast-bert) (4.42.0)\n",
      "Requirement already satisfied, skipping upgrade: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers>=2.3.0->fast-bert) (0.0.38)\n",
      "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.3.0->fast-bert) (2020.2.20)\n",
      "Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=2.3.0->fast-bert) (3.0.12)\n",
      "Requirement already satisfied, skipping upgrade: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers>=2.3.0->fast-bert) (0.1.85)\n",
      "Requirement already satisfied, skipping upgrade: torchvision in /usr/local/lib/python3.6/dist-packages (from pytorch-lamb->fast-bert) (0.6.0.dev20200128)\n",
      "Requirement already satisfied, skipping upgrade: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-lamb->fast-bert) (1.5.0.dev20200128)\n",
      "Requirement already satisfied, skipping upgrade: ruamel.yaml in /usr/local/lib/python3.6/dist-packages (from python-box->fast-bert) (0.16.10)\n",
      "Requirement already satisfied, skipping upgrade: toml in /usr/local/lib/python3.6/dist-packages (from python-box->fast-bert) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->fast-bert) (2019.3)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->fast-bert) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->fast-bert) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->fast-bert) (1.25.8)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->fast-bert) (2019.11.28)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->fast-bert) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy->fast-bert) (1.4.0)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn->fast-bert) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn->fast-bert) (0.14.1)\n",
      "Requirement already satisfied, skipping upgrade: botocore<1.16.0,>=1.15.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers>=2.3.0->fast-bert) (1.15.15)\n",
      "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers>=2.3.0->fast-bert) (0.9.5)\n",
      "Requirement already satisfied, skipping upgrade: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers>=2.3.0->fast-bert) (0.3.3)\n",
      "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.3.0->fast-bert) (7.0)\n",
      "Requirement already satisfied, skipping upgrade: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision->pytorch-lamb->fast-bert) (7.0.0)\n",
      "Requirement already satisfied, skipping upgrade: ruamel.yaml.clib>=0.1.2; platform_python_implementation == \"CPython\" and python_version < \"3.9\" in /usr/local/lib/python3.6/dist-packages (from ruamel.yaml->python-box->fast-bert) (0.2.0)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy->fast-bert) (2.1.0)\n",
      "Requirement already satisfied, skipping upgrade: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.15->boto3->transformers>=2.3.0->fast-bert) (0.15.2)\n",
      "Installing collected packages: fast-bert\n",
      "  Attempting uninstall: fast-bert\n",
      "    Found existing installation: fast-bert 1.6.2\n",
      "    Uninstalling fast-bert-1.6.2:\n",
      "      Successfully uninstalled fast-bert-1.6.2\n",
      "Successfully installed fast-bert-1.6.4\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade fast-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import apex\n",
    "from box import Box\n",
    "\n",
    "from fast_bert.data_cls import BertDataBunch\n",
    "from fast_bert.learner_cls import BertLearner\n",
    "from fast_bert.metrics import accuracy, roc_auc, fbeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_TYPE = 'albert'\n",
    "MODEL_NAME = 'albert-large-v2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('data/splited/')\n",
    "LABEL_PATH = Path('label/')\n",
    "\n",
    "MODEL_PATH = Path('bert_models/')\n",
    "LOG_PATH = Path('bert_logs/')\n",
    "OUTPUT_DIR = MODEL_PATH / ('output.%s' % MODEL_NAME)\n",
    "\n",
    "MODEL_PATH.mkdir(exist_ok=True)\n",
    "LOG_PATH.mkdir(exist_ok=True)\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FILE = 'train_900000.csv'\n",
    "VAL_FILE = 'val_100000.csv'\n",
    "LABEL_FILE = 'labels.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU & device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "multi_gpu = torch.cuda.device_count() > 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Box({\n",
    "    'data_path': DATA_PATH,\n",
    "    'label_path': LABEL_PATH,\n",
    "    'model_path': MODEL_PATH,\n",
    "    'log_path': LOG_PATH,\n",
    "    'output_dir': OUTPUT_DIR,\n",
    "    'finetuned_path': None,\n",
    "    \n",
    "    'model_type': MODEL_TYPE,\n",
    "    'model_name': MODEL_NAME,\n",
    "    'do_lower_case': True,\n",
    "\n",
    "    'num_train_epochs': 3,\n",
    "    'learning_rate': 3e-5,\n",
    "    'max_seq_length': 256,\n",
    "    'train_batch_size': 8,\n",
    "    'multi_label': False,\n",
    "    \n",
    "    'device': device,\n",
    "    'multi_gpu': multi_gpu,\n",
    "    \n",
    "    'warmup_steps': 500,\n",
    "    'fp16': True,\n",
    "    'logging_steps': 0,\n",
    "    \n",
    "    'schedule_type': 'warmup_cosine',\n",
    "    'optimizer_type': 'lamb',\n",
    "    'warmup_proportion': 0.002,\n",
    "    'local_rank': -1,\n",
    "    'gradient_accumulation_steps': 1,\n",
    "    'loss_scale': 128\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create a DataBunch object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "databunch = BertDataBunch(args.data_path, args.label_path,\n",
    "                          tokenizer=args.model_name,\n",
    "                          train_file=TRAIN_FILE,\n",
    "                          val_file=VAL_FILE,\n",
    "                          label_file=LABEL_FILE,\n",
    "                          text_col='text',\n",
    "                          label_col='label',\n",
    "                          batch_size_per_gpu=args.train_batch_size,\n",
    "                          max_seq_length=args.max_seq_length,\n",
    "                          multi_gpu=args.multi_gpu,\n",
    "                          multi_label=args.multi_label,\n",
    "                          model_type=args.model_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create a Learner Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "metrics = [\n",
    "    {'name': 'accuracy', 'function': accuracy}\n",
    "    # {'name': 'roc_auc', 'function': roc_auc}, # ValueError: Found input variables with inconsistent numbers of samples: [300000, 600000]\n",
    "    # {'name': 'fbeta', 'function': fbeta}, # RuntimeError: The size of tensor a (2) must match the size of tensor b (300000) at non-singleton dimension 1\n",
    "]\n",
    "\n",
    "learner = BertLearner.from_pretrained_model(databunch,\n",
    "                                            pretrained_path=args.model_name,\n",
    "                                            metrics=metrics,\n",
    "                                            device=args.device,\n",
    "                                            logger=logger,\n",
    "                                            output_dir=args.output_dir,\n",
    "                                            finetuned_wgts_path=args.finetuned_path,\n",
    "                                            warmup_steps=args.warmup_steps,\n",
    "                                            multi_gpu=args.multi_gpu,\n",
    "                                            is_fp16=args.fp16,\n",
    "                                            multi_label=args.multi_label,\n",
    "                                            logging_steps=args.logging_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='1' class='' max='3', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      33.33% [1/3 5:02:55<10:05:50]\n",
       "    </div>\n",
       "    \n",
       "\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='42774' class='' max='56250', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      76.04% [42774/56250 3:40:21<1:09:25]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:1334: UserWarning: This overload of add_ is deprecated:\n",
      "add_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "add_(Tensor other, Number alpha)\n",
      "/pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:1550: UserWarning: This overload of addcmul_ is deprecated:\n",
      "addcmul_(Number value, Tensor tensor1, Tensor tensor2)\n",
      "Consider using one of the following signatures instead:\n",
      "addcmul_(Tensor tensor1, Tensor tensor2, Number value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='3125' class='' max='3125', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [3125/3125 10:22<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:229: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n"
     ]
    }
   ],
   "source": [
    "learner.fit(epochs=args.num_train_epochs,\n",
    "            lr=args.learning_rate,\n",
    "            validate=True, # Evaluate the model after each epoch\n",
    "            schedule_type=args.schedule_type,\n",
    "            optimizer_type=args.optimizer_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fast_bert.prediction import BertClassificationPredictor\n",
    "\n",
    "MODEL_PATH = OUTPUT_DIR / 'model_out'\n",
    "\n",
    "predictor = BertClassificationPredictor(model_path=str(MODEL_PATH),\n",
    "                                        label_path=str(LABEL_PATH),\n",
    "                                        multi_label=args.multi_label,\n",
    "                                        model_type=args.model_type,\n",
    "                                        do_lower_case=args.do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_CSV = Path('bert_scores') / ('output_%s.csv' % args.model_type)\n",
    "\n",
    "test_df = pd.read_csv(DATA_PATH / VAL_FILE)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = predictor.predict_batch(test_df.text.tolist())\n",
    "pd.DataFrame(output).to_csv(OUTPUT_CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRUES, PREDS, PROBS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trues = np.asarray([int(v == 'phishing') for v in test_df.label.values])\n",
    "trues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.asarray([int(pred[0][0] == 'phishing') for pred in output])\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = np.asarray([pred[0][1] if pred[0][0] == 'phishing' else pred[1][1] for pred in output])\n",
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sklearn.metrics.classification_report(trues, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = sklearn.metrics.confusion_matrix(trues, preds).ravel()\n",
    "\n",
    "accuracy = sklearn.metrics.accuracy_score(trues, preds)\n",
    "balance_accuracy = sklearn.metrics.balanced_accuracy_score(trues, preds)\n",
    "precision, recall, f1, _ = sklearn.metrics.precision_recall_fscore_support(trues, preds, beta=1.0, average='binary')\n",
    "jaccard = sklearn.metrics.jaccard_score(trues, preds)\n",
    "matthews_corrcoef = sklearn.metrics.matthews_corrcoef(trues, preds)\n",
    "\n",
    "hamming_loss = sklearn.metrics.hamming_loss(trues, preds)\n",
    "log_loss = sklearn.metrics.log_loss(trues, preds)\n",
    "zero_one_loss = sklearn.metrics.zero_one_loss(trues, preds)\n",
    "brier_score_loss = sklearn.metrics.brier_score_loss(trues, probs)\n",
    "\n",
    "print('score')\n",
    "print('accuracy:', accuracy)\n",
    "print('balance_accuracy:', balance_accuracy)\n",
    "print('precision:', precision)\n",
    "print('recall:', recall)\n",
    "print('f1:', f1)\n",
    "print('jaccard:', jaccard)\n",
    "print('matthews_corrcoef:', matthews_corrcoef)\n",
    "\n",
    "print('\\nloss')\n",
    "print('hamming_loss:', hamming_loss)\n",
    "print('log_loss:', log_loss)\n",
    "print('zero_one_loss:', zero_one_loss)\n",
    "print('brier_score_loss:', brier_score_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
